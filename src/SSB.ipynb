{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In celebration of Halloween 2017, Kaggle launched a playground competition titled “Spooky Author Identification” which tasked participants with identifying  horror authors from their writings. The dataset for this NLP classification task consists of excerpts from horror stories by Edgar Allan Poe [EAP], Mary Shelley [MWS], and HP Lovecraft [HPL] and is pre-split into a training set (19,580 samples) and testing set (8,393 samples). An entry in either dataset consists of an id (unique identifier for the excerpt), and a single sentence of varied length from one of the three authors (the training set includes the name of the author). Example training observation:\n",
    "\n",
    "- \"id22965\",\"A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.\",\"MWS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import simple_BERT as SSB\n",
    "importlib.reload(SSB)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.10.0\n",
      "TF-Hub version:  0.13.0\n",
      "Eager mode enabled:  True\n",
      "WARNING:tensorflow:From /Users/bisramr/Documents/Cooper Stuff/Advanced-NLP/author-identification/src/spooky_simple_BERT.py:22: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Metal device set to: Apple M2\n",
      "GPU available:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 13:24:57.992126: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-27 13:24:57.992367: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tokenization \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "notebookstart = time.time()\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "SSB.print_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to this problem leverages the popular BERT (Bidirectional Encoder Representations from Transformers) language model by fine tuning it for this specific classification task. The BERT model used was pulled from tensorflow hub and follows what the original paper denotes as BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). L is the number of transformer blocks (hidden layers), H is the hidden size, and A is the number of self attention heads. The weights of this model are those released by the original BERT authors. This model has been pre-trained for English on the Wikipedia and BooksCorpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 19:30:36.739583: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-27 19:30:36.740259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 3.62 s, total: 32.7 s\n",
      "Wall time: 58.6 s\n"
     ]
    }
   ],
   "source": [
    "# Import the bert model from tensorflow hub\n",
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: 19579 Rows, 3 Columns\n",
      "Test Shape: 8392 Rows, 2 Columns\n",
      "Train Sequence Length - Mean 148.7 +/- 107.7, Max 4663.0, Min 21.0\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 64*3\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "SEED = 42\n",
    "NROWS = None\n",
    "TEXTCOL = \"text\"\n",
    "TARGETCOL = \"author\"\n",
    "NCLASS = 3\n",
    "\n",
    "dir = '../spooky-author-identification'\n",
    "\n",
    "train = pd.read_csv(f\"{dir}/train.csv\")\n",
    "test = pd.read_csv(f\"{dir}/test.csv\")\n",
    "testdex = test.id\n",
    "submission = pd.read_csv(f\"{dir}/sample_submission.csv\")\n",
    "\n",
    "sub_cols = submission.columns\n",
    "\n",
    "print(\"Train Shape: {} Rows, {} Columns\".format(*train.shape))\n",
    "print(\"Test Shape: {} Rows, {} Columns\".format(*test.shape))\n",
    "\n",
    "length_info = [len(x) for x in np.concatenate([train[TEXTCOL].values, test[TEXTCOL].values])]\n",
    "print(\"Train Sequence Length - Mean {:.1f} +/- {:.1f}, Max {:.1f}, Min {:.1f}\".format(np.mean(length_info), np.std(length_info), np.max(length_info), np.min(length_info)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "231\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "print(train[TEXTCOL].values[0])\n",
    "print(len(train[TEXTCOL].values[0]))\n",
    "print(length_info[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/59654175/how-to-get-the-vocab-file-for-bert-tokenizer-from-tf-hub\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how was your day? My name is Ravindra lolokfehfe\n",
      "['hello', ',', 'how', 'was', 'your', 'day', '?', 'my', 'name', 'is', 'ravi', '##ndra', 'lo', '##lok', '##fe', '##h', '##fe']\n",
      "['hello', ',', 'how', 'was', 'your', 'day', '?', 'my', 'name', 'is', 'ravi', '##ndra', 'lo', '##lok', '##fe', '##h', '##fe']\n",
      "['[CLS]', 'hello', ',', 'how', 'was', 'your', 'day', '?', 'my', 'name', 'is', 'ravi', '##ndra', 'lo', '##lok', '##fe', '##h', '##fe', '[SEP]']\n",
      "[101, 7592, 1010, 2129, 2001, 2115, 2154, 1029, 2026, 2171, 2003, 16806, 17670, 8840, 29027, 7959, 2232, 7959, 102]\n",
      "[101, 7592, 1010, 2129, 2001, 2115, 2154, 1029, 2026, 2171, 2003, 16806, 17670, 8840, 29027, 7959, 2232, 7959, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/microsoft/SDNet/blob/master/bert_vocab_files/bert-base-uncased-vocab.txt\n",
    "text = \"Hello, how was your day? My name is Ravindra lolokfehfe\"\n",
    "print(text)\n",
    "text = tokenizer.tokenize(text)\n",
    "print(text)\n",
    "text = text[:MAX_LEN-2]\n",
    "print(text)\n",
    "input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "print(input_sequence)\n",
    "pad_len = MAX_LEN - len(input_sequence)\n",
    "tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "print(tokens)\n",
    "tokens += [0] * pad_len\n",
    "pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "segment_ids = [0] * MAX_LEN\n",
    "print(tokens)\n",
    "print(pad_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training and testing inputs with bert encoder\n",
    "train_input = SSB.bert_encode(train[TEXTCOL].values, tokenizer, max_len=MAX_LEN)\n",
    "test_input = SSB.bert_encode(test[TEXTCOL].values, tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "# Ditionary with author name as keys and mapping integer as value\n",
    "label_mapper = {name: i for i,name in enumerate(set(train[TARGETCOL].values))}\n",
    "# List of author names converted to integer with mapper\n",
    "num_label = np.vectorize(label_mapper.get)(train[TARGETCOL].values)\n",
    "# num_label converted from integers to one hot encoding [2 --> [0 0 1]]\n",
    "train_labels = to_categorical(num_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2009  2196  2320  4158  2000  2033  2008  1996 11865 29256  2453\n",
      "  2022  1037  8210  6707  1012   102     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "19579 192\n",
      "19579 192\n",
      "19579 192\n",
      "{'HPL': 0, 'MWS': 1, 'EAP': 2}\n",
      "19579 [2 1 1 2 0 0 2 1 2 1 2 0]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0][1])\n",
    "print(len(train_input[0]), len(train_input[0][0]))\n",
    "print(len(train_input[1]), len(train_input[0][1]))\n",
    "print(len(train_input[2]), len(train_input[1][1]))\n",
    "print(label_mapper)\n",
    "print(len(num_label), num_label[8:20])\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  101,  2023,  2832, ...,     0,     0,     0],\n",
      "       [  101,  2009,  2196, ...,     0,     0,     0],\n",
      "       [  101,  1999,  2010, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 14736,  2015, ...,     0,     0,     0],\n",
      "       [  101,  2005,  2019, ...,     0,     0,     0],\n",
      "       [  101,  2002,  4201, ...,     0,     0,     0]]), array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "print(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       [(None, 1024),       335141889   ['input_word_ids[0][0]',         \n",
      "                                 (None, 192, 1024)]               'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1024)        0           ['keras_layer[0][1]']            \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            3075        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335,144,964\n",
      "Trainable params: 335,144,963\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bisramr/tensorflow-metal/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build model and print summary\n",
    "model = SSB.build_model(bert_layer, NCLASS, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.8152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 20:53:52.830221: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1958/1958 [==============================] - 5053s 3s/step - loss: 0.4484 - accuracy: 0.8152 - val_loss: 0.4312 - val_accuracy: 0.8304\n",
      "Epoch 2/15\n",
      "1958/1958 [==============================] - 5119s 3s/step - loss: 0.1384 - accuracy: 0.9498 - val_loss: 0.3480 - val_accuracy: 0.8764\n",
      "Epoch 3/15\n",
      "1958/1958 [==============================] - 5223s 3s/step - loss: 0.0373 - accuracy: 0.9879 - val_loss: 0.4528 - val_accuracy: 0.8769\n",
      "Epoch 4/15\n",
      " 313/1958 [===>..........................] - ETA: 1:08:31 - loss: 0.0208 - accuracy: 0.9940"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m'\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m es \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,\n\u001b[1;32m      3\u001b[0m                              patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, baseline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                              restore_best_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m train_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      8\u001b[0m     train_input, train_labels,\n\u001b[1;32m      9\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m     11\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint, es],\n\u001b[1;32m     12\u001b[0m     batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch')\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n",
    "                             patience=4, verbose=1, mode='min', baseline=None,\n",
    "                             restore_best_weights=False)\n",
    "\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint, es],\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 00:05:57.600170: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 694s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8392, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8392, 4)\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame(test_pred, columns=label_mapper.keys())\n",
    "submission['id'] = testdex\n",
    "\n",
    "submission = submission[sub_cols]\n",
    "submission.to_csv('submission_bert.csv', index=False)\n",
    "print(submission.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: MWS\n",
    "1: EAP\n",
    "2: HPL\n",
    "3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id       EAP       HPL           MWS author\n",
      "0     id02310  0.000774  0.007881  9.913454e-01    MWS\n",
      "1     id24541  0.998308  0.000174  1.518073e-03    EAP\n",
      "2     id00134  0.000174  0.999821  5.248776e-06    HPL\n",
      "3     id27757  0.980690  0.018335  9.744923e-04    EAP\n",
      "4     id04081  0.204278  0.752165  4.355776e-02    HPL\n",
      "...       ...       ...       ...           ...    ...\n",
      "8387  id11749  0.038030  0.009188  9.527821e-01    MWS\n",
      "8388  id10526  0.910455  0.021065  6.848040e-02    EAP\n",
      "8389  id13477  0.999748  0.000169  8.373301e-05    EAP\n",
      "8390  id13761  0.312758  0.047380  6.398616e-01    MWS\n",
      "8391  id04282  0.000062  0.999938  9.586085e-08    HPL\n",
      "\n",
      "[8392 rows x 5 columns]\n",
      "           id       EAP       HPL       MWS author\n",
      "0     id02310  0.033902  0.009306  0.956793    MWS\n",
      "1     id24541  0.998126  0.001162  0.000712    EAP\n",
      "2     id00134  0.023430  0.972711  0.003859    HPL\n",
      "3     id27757  0.708356  0.280364  0.011279    EAP\n",
      "4     id04081  0.932844  0.035967  0.031188    EAP\n",
      "...       ...       ...       ...       ...    ...\n",
      "8387  id11749  0.772756  0.017177  0.210068    EAP\n",
      "8388  id10526  0.068000  0.017017  0.914984    MWS\n",
      "8389  id13477  0.991073  0.004784  0.004143    EAP\n",
      "8390  id13761  0.017223  0.007781  0.974996    MWS\n",
      "8391  id04282  0.009337  0.990200  0.000463    HPL\n",
      "\n",
      "[8392 rows x 5 columns]\n",
      "           id       EAP       HPL       MWS author author2\n",
      "0     id02310  0.033902  0.009306  0.956793    MWS     MWS\n",
      "1     id24541  0.998126  0.001162  0.000712    EAP     EAP\n",
      "2     id00134  0.023430  0.972711  0.003859    HPL     HPL\n",
      "3     id27757  0.708356  0.280364  0.011279    EAP     EAP\n",
      "4     id04081  0.932844  0.035967  0.031188    EAP     HPL\n",
      "...       ...       ...       ...       ...    ...     ...\n",
      "8387  id11749  0.772756  0.017177  0.210068    EAP     MWS\n",
      "8388  id10526  0.068000  0.017017  0.914984    MWS     EAP\n",
      "8389  id13477  0.991073  0.004784  0.004143    EAP     EAP\n",
      "8390  id13761  0.017223  0.007781  0.974996    MWS     MWS\n",
      "8391  id04282  0.009337  0.990200  0.000463    HPL     HPL\n",
      "\n",
      "[8392 rows x 6 columns]\n",
      "1258\n",
      "1325\n",
      "857\n"
     ]
    }
   ],
   "source": [
    "to_submit = pd.read_csv('submission_bert.csv')\n",
    "# print(to_submit)\n",
    "to_submit['author'] = to_submit.iloc[:, 1:].idxmax(axis=1)\n",
    "print(to_submit)\n",
    "checking = pd.read_csv(f\"{dir}/sub_fe.csv\")\n",
    "checking2 = pd.read_csv(f\"{dir}/grammar_results.csv\")\n",
    "\n",
    "checking['author'] = checking.iloc[:, 1:].idxmax(axis=1)\n",
    "checking2['author'] = checking2.iloc[:, 1:].idxmax(axis=1)\n",
    "\n",
    "print(checking)\n",
    "checking['author2'] = to_submit['author']\n",
    "print(checking)\n",
    "\n",
    "def compare_results(df1, df2):\n",
    "    list1 = list(df1.author)\n",
    "    list2 = list(df2.author)\n",
    "    discrepencies = 0\n",
    "\n",
    "    for i in zip(list1, list2):\n",
    "        if i[0] != i[1]:\n",
    "            discrepencies += 1\n",
    "\n",
    "    return discrepencies\n",
    "\n",
    "print(compare_results(checking, to_submit))\n",
    "print(compare_results(checking2, to_submit))\n",
    "print(compare_results(checking2, checking))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tensorflow-metal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cb3f970edcee8cb5b87b188c4c9539854f72a7c2b4d7860930ec7985dcd0979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
